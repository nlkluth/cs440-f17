{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nathan Kluth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, you will find code that trains neural networks of various numbers of hidden layers and units in each hidden layer and returns results as specified below. There is one example using a regression problem, and one using a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Brought in from outisde file\n",
    "import neuralnetworks as nns\n",
    "import scaledconjugategradient as scg\n",
    "import mlutils as ml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up `trainNNs`. \n",
    "\n",
    "* `X` is a matrix of input data of shape `nSamples x nFeatures`\n",
    "* `T` is a matrix of target data of shape `nSamples x nOutputs`\n",
    "* `trainFraction` is fraction of samples to use as training data. 1-`trainFraction` is number of samples for testing data\n",
    "* `hiddenLayerStructures` is list of network architectures. For example, to test two networks, one with one hidden layer of 20 units, and one with 3 hidden layers with 5, 10, and 20 units in each layer, this argument would be `[[20], [5, 10, 20]]`.\n",
    "* `numberRepetitions` is number of times to train a neural network.  Calculate training and testing average performance (two separate averages) of this many training runs.\n",
    "* `numberIterations` is the number of iterations to run the scaled conjugate gradient algorithm when a neural network is trained.\n",
    "* `classify` is set to `True` if you are doing a classification problem, in which case `T` must be a single column of target class integers.\n",
    "\n",
    "This function returns `results` which is list with one element for each network structure tested.  Each element is a list containing \n",
    "* the hidden layer structure (as a list),\n",
    "* a list of training data performance for each repetition, \n",
    "* a list of testing data performance for each repetition, and\n",
    "* the number of seconds it took to run this many repetitions for this network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trainNNs(X, T, trainFraction, hiddenLayerStructures, numberRepetitions, numberIterations, classify=False):\n",
    "    results = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for layer in hiddenLayerStructures:\n",
    "        trainDataPerf = []\n",
    "        testDataPerf = []\n",
    "        \n",
    "        for rep in range(numberRepetitions):\n",
    "            # partition data set into train/test\n",
    "            Xtrain,Ttrain,Xtest,Ttest = ml.partition(X, T, (trainFraction, 1-trainFraction), classify);\n",
    "\n",
    "            # create a neural network of the given structure\n",
    "            nnet = nns.NeuralNetwork(Xtrain.shape[1], layer, Ttrain.shape[1]);\n",
    "\n",
    "            # train it for numberIterations\n",
    "            nnet.train(Xtrain, Ttrain, numberIterations)\n",
    "\n",
    "            # use the trained network to produce outputs for the training and for the testing sets\n",
    "            Y = nnet.use(Xtrain)\n",
    "            Ytest, _ = nnet.use(Xtest, allOutputs=True)\n",
    "\n",
    "            trainingResult = None\n",
    "            testingResult = None\n",
    "            if (classify):\n",
    "                # calculate the fraction of samples incorrectly classififed for training and testing sets. \n",
    "                values,counts = np.unique(Ttrain, return_counts=True)\n",
    "                values2,counts2 = np.unique(Ttest, return_counts=True)\n",
    "                \n",
    "                trainingResult = (counts / Ttrain.shape[0]).tolist()\n",
    "                testingResult = (counts2 / Ttest.shape[0]).tolist()\n",
    "                \n",
    "                # add the training and testing preformance to a collection (such as a list) for this network structure\n",
    "                trainDataPerf = trainDataPerf + trainingResult\n",
    "                testDataPerf = testDataPerf + testingResult\n",
    "            else:\n",
    "                # calculate RMSE of trainng and testing sets\n",
    "                trainingResult = np.sqrt(np.mean((Y-Ttrain)**2))\n",
    "                testingResult = np.sqrt(np.mean((Ytest-Ttest)**2))\n",
    "                \n",
    "                # add the training and testing preformance to a collection (such as a list) for this network structure\n",
    "                trainDataPerf = trainDataPerf + [trainingResult]\n",
    "                testDataPerf = testDataPerf + [testingResult]\n",
    "            \n",
    "        # add to a collection iof all results the hidden layer structure, \n",
    "        # lists of training performance and testing performance, and seconds taken to do these repeitions\n",
    "        results += [[layer, trainDataPerf, testDataPerf, time.time() - t0]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few helper functions for taking a look at the results:\n",
    "\n",
    "`summarize(results)` returns a list of lists like `results` but with the list of training performances replaced by their mean and the list of testing performances replaced by their mean. \n",
    "\n",
    "`bestNetwork(summary)` takes the output of `summarize(results)` and returns the best element of `results`, determined by the element that has the smallest test performance.\n",
    "\n",
    "* `summary = summarize(results)` where `results` is returned by `trainNNs` and `summary` is like `results` with the training and testing performance lists replaced by their means\n",
    "* `best = bestNetwork(summary)` where `summary` is returned by `summarize` and `best` is the best element of `summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "\n",
    "def summarize(results): \n",
    "    summary = cp.deepcopy(results)\n",
    "    \n",
    "    for summ in summary:\n",
    "        summ[1] = float(sum(summ[1])) / len(summ[1])\n",
    "        summ[2] = float(sum(summ[2])) / len(summ[2])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary = summarize([[[1,1], [1.2, 1.3, 1.4], [2.2, 2.3, 2.4], 0.5], [[2,2,2], [4.4, 4.3, 4.2], [6.5, 6.4, 6.3], 0.6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bestNetwork(summary):\n",
    "    best = float('inf')\n",
    "    bestIndex = -1\n",
    "    for (index, network) in enumerate(summary):\n",
    "        if (network[2] < best):\n",
    "            best = network[2]\n",
    "            bestIndex = index\n",
    "            \n",
    "    return summary[bestIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1], 1.3, 2.3000000000000003, 0.5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestNetwork(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.arange(10).reshape((-1,1))\n",
    "T = X + 1 + np.random.uniform(-1, 1, ((10,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJh4v8D0ignsC7uUscAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f4e0278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, T, 'o-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70710678,  0.28408674,  0.16213987,  0.1621314 ,  0.12025594,\n",
       "        0.11984705,  0.11889843,  0.11883879,  0.11878718,  0.11878088,\n",
       "        0.11878088,  0.11827175,  0.11816814,  0.11808812,  0.11804607,\n",
       "        0.1180408 ,  0.11803604,  0.11786684,  0.11786531,  0.11761915,\n",
       "        0.11761754,  0.1175882 ,  0.11756965,  0.11756856,  0.11756829,\n",
       "        0.11755689,  0.11755687,  0.11755675,  0.11755588,  0.11755585,\n",
       "        0.11755583,  0.11755566,  0.1175556 ,  0.11755559,  0.11755558,\n",
       "        0.11755558,  0.11755556,  0.11755556,  0.11755556,  0.11755554,\n",
       "        0.11755553,  0.11755552,  0.11755552,  0.11755534,  0.11755525,\n",
       "        0.11755524,  0.11755522,  0.11755522,  0.11755522,  0.11755519,\n",
       "        0.11755519,  0.11755519,  0.11755516,  0.11755516,  0.11755516,\n",
       "        0.11755515,  0.11755515,  0.11755515,  0.11755515,  0.11755515,\n",
       "        0.11755514,  0.11755514,  0.11755514,  0.11755514,  0.11755511,\n",
       "        0.11755511,  0.11755511,  0.11755508,  0.11755508,  0.11755508,\n",
       "        0.11755508,  0.11755508,  0.11755508,  0.11755508,  0.11755508,\n",
       "        0.11755508,  0.11755508,  0.11755508,  0.117555  ,  0.11755499,\n",
       "        0.11755499,  0.11755499,  0.11755499,  0.11755499,  0.11755499,\n",
       "        0.11755498,  0.11755498,  0.11755498,  0.11755498,  0.11755498,\n",
       "        0.11755498,  0.11755498,  0.11755497,  0.11755497,  0.11755497,\n",
       "        0.11755497,  0.11755497,  0.11755497,  0.11755497,  0.11755496,\n",
       "        0.11755496])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = nns.NeuralNetwork(X.shape[1], 2, T.shape[1])\n",
    "nnet.train(X, T, 100)\n",
    "nnet.getErrorTrace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70710678,  0.57180063,  0.57180063,  0.46210517,  0.31659347,\n",
       "        0.24528895,  0.14880738,  0.13694919,  0.13204544,  0.12569857,\n",
       "        0.11649055,  0.11590391,  0.11569306,  0.11566122,  0.11562937,\n",
       "        0.11551468,  0.11505223,  0.11464121,  0.11449277,  0.11437105,\n",
       "        0.11413091,  0.11391687,  0.11387749,  0.11384074,  0.11380551,\n",
       "        0.11366933,  0.11363948,  0.11341866,  0.11295234,  0.11281427,\n",
       "        0.1121272 ,  0.10893423,  0.10890256,  0.10874585,  0.10801654,\n",
       "        0.1071216 ,  0.10579049,  0.10578391,  0.10575908,  0.10516641,\n",
       "        0.10508071,  0.10488666,  0.10478763,  0.10472251,  0.1046463 ,\n",
       "        0.10464091,  0.10460702,  0.10459603,  0.10457686,  0.10449735,\n",
       "        0.10442185,  0.10423975,  0.10418259,  0.10412981,  0.1038987 ,\n",
       "        0.10315412,  0.1030837 ,  0.1029208 ,  0.10291564,  0.10270774,\n",
       "        0.10265041,  0.10264459,  0.10262698,  0.10249307,  0.10248554,\n",
       "        0.102406  ,  0.10237379,  0.10222272,  0.10211548,  0.10202967,\n",
       "        0.10189944,  0.10183365,  0.10181795,  0.10180708,  0.10177573,\n",
       "        0.10176865,  0.10173204,  0.10172315,  0.10172128,  0.1016962 ,\n",
       "        0.10167773,  0.10161956,  0.10137563,  0.10116089,  0.10109508,\n",
       "        0.10063055,  0.10058769,  0.10057331,  0.10056589,  0.10056589,\n",
       "        0.10056589,  0.10056589,  0.10056589,  0.10056589,  0.10056589,\n",
       "        0.10056589,  0.10056589,  0.10056589,  0.10056589,  0.10056589,\n",
       "        0.10056589])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = nns.NeuralNetwork(X.shape[1], [5, 5, 5], T.shape[1])\n",
    "nnet.train(X, T, 100)\n",
    "nnet.getErrorTrace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  [0.32559415354725746,\n",
       "   0.35128734805052325,\n",
       "   0.42726251359300305,\n",
       "   0.37796141482226986,\n",
       "   0.30200554616264091],\n",
       "  [1.0610948863220153,\n",
       "   0.81362166868474495,\n",
       "   0.82828168910198607,\n",
       "   0.46679577714257336,\n",
       "   1.1479178534834495],\n",
       "  0.1255168914794922],\n",
       " [10,\n",
       "  [0.30879085833882286,\n",
       "   0.25304698956899441,\n",
       "   0.30236597426110412,\n",
       "   0.23314699373240921,\n",
       "   0.18618393189099372],\n",
       "  [1.2920149331307409,\n",
       "   0.82968556570037844,\n",
       "   1.2311098851960414,\n",
       "   0.83273766166494945,\n",
       "   1.3759975702312004],\n",
       "  0.23699688911437988],\n",
       " [[10, 10],\n",
       "  [0.12138377372434454,\n",
       "   0.23165055934117645,\n",
       "   0.14832568538559424,\n",
       "   0.25451380023971698,\n",
       "   0.25917083459781282],\n",
       "  [2.5406995061033681,\n",
       "   1.0816817957414753,\n",
       "   1.4484837337617731,\n",
       "   0.9951388738587108,\n",
       "   0.89465270687672172],\n",
       "  0.390239953994751]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainNNs(X, T, 0.8, [2, 10, [10, 10]], 5, 100, classify=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = trainNNs(X, T, 0.8, [0, 1, 2, 10, [10, 10], [5, 5, 5, 5], [2]*5], 50, 400, classify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.4653129554439147, 0.5624970085904204, 0.024330854415893555],\n",
       " [1, 0.4387512270634915, 0.7247390318095657, 1.6899747848510742],\n",
       " [2, 0.29426209670360354, 1.2917976612313558, 6.086277961730957],\n",
       " [10, 0.02932988379688262, 1.6148491361698531, 9.61492109298706],\n",
       " [[10, 10], 0.018694059140247302, 1.6187905412919559, 15.430840969085693],\n",
       " [[5, 5, 5, 5], 0.13498450919911772, 1.2277986716140572, 24.376418828964233],\n",
       " [[2, 2, 2, 2, 2], 0.29428828253752715, 1.340021719418809, 33.97822403907776]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.4653129554439147, 0.5624970085904204, 0.024330854415893555]\n",
      "Hidden Layers 0 Average RMSE Training 0.47 Testing 0.56 Took 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "best = bestNetwork(summarize(results))\n",
    "print(best)\n",
    "print('Hidden Layers {} Average RMSE Training {:.2f} Testing {:.2f} Took {:.2f} seconds'.format(*best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for Regression Experiment\n",
    "\n",
    "From the UCI Machine Learning Repository, download the [Appliances energy prediction](http://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction) data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def readEnergyFile(fileName):\n",
    "    file = open(fileName, 'r')\n",
    "    formatted = file.read().strip().replace('\"','')\n",
    "    newFile = open('newfile.csv','w') \n",
    "    newFile.write(formatted)\n",
    "    \n",
    "    readFormatted = open('newfile.csv', 'r')\n",
    "    header = readFormatted.readline()\n",
    "    fullNames = header.strip().replace('\"', '').split(',')\n",
    "    names = fullNames[1:len(fullNames)-2]\n",
    "\n",
    "    data = np.loadtxt(readFormatted, delimiter=',', usecols=1+np.arange(26))\n",
    "    return (names, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'energydata_complete.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bf1ed81858e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadEnergyFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"energydata_complete.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-7a4dedd23c72>\u001b[0m in \u001b[0;36mreadEnergyFile\u001b[0;34m(fileName)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadEnergyFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mformatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnewFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'newfile.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'energydata_complete.csv'"
     ]
    }
   ],
   "source": [
    "names, data = readEnergyFile(\"energydata_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the first two columns, labelled `Appliances` and `lights` as the target variables, and the remaining 24 columns as the input features.  So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "\n",
    "targetColumn = names.index(\"Appliances\")\n",
    "targetColumn2 = names.index(\"lights\")\n",
    "XColumns = np.arange(26)\n",
    "XColumns = np.delete(XColumns, targetColumn)\n",
    "XColumns = np.delete(XColumns, targetColumn2)\n",
    "\n",
    "Tenergy = data[:, [targetColumn, targetColumn2]] #.reshape((-1, 1)) # to keep 2-d matrix form\n",
    "Xenergy = data[:, XColumns]\n",
    "\n",
    "Xnames = cp.deepcopy(names)\n",
    "Xnames.remove('Appliances')\n",
    "Xnames.remove('lights')\n",
    "\n",
    "Tnames = ['Appliances', 'lights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xenergy.shape, Tenergy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm going to train a bunch of networks and see which is best so I can use that to train the Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = trainNNs(Xenergy, Tenergy, 0.8, [0, 5, [5, 5], [10, 10]], 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bestNetwork(summarize(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results2 = trainNNs(Xenergy, Tenergy, 0.8, [1, 5, [5, 5], 10, [3, 3, 3], [8, 8], 15, [2, 2, 2, 2], [10, 10], [5, 5, 5]], 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarize(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bestNetwork(summarize(results2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger numbers of layers and units may do the best on training data, but not on testing data. Why?\n",
    "\n",
    "I *think* is has to do with the small amount of training data - it isn't enough to characterize the test data sufficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BXtrain,BTtrain,BXtest,BTtest = ml.partition(Xenergy, Tenergy, (0.8, 0.2), classification=False);\n",
    "Bestnnet = nns.NeuralNetwork(BXtrain.shape[1], 10, BTtrain.shape[1]); # again using best network\n",
    "Bestnnet.train(BXtrain, BTtrain, 100)\n",
    "\n",
    "Ptrain,Prtrain = Bestnnet.use(BXtrain,allOutputs=True)\n",
    "Ptest,Prtest = Bestnnet.use(BXtest,allOutputs=True)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.hstack((BTtrain,Ptrain)), '.')\n",
    "plt.legend(('Actual','Predicted'))\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.hstack((BTtest,Ptest)), '.')\n",
    "plt.legend(('Actual','Predicted'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure where the green comes from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for Classification Experiment\n",
    "\n",
    "From the UCI Machine Learning Repository, download the [Anuran Calls (MFCCs)](http://archive.ics.uci.edu/ml/datasets/Anuran+Calls+%28MFCCs%29) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%!wget` not found.\n",
      "ERROR:root:Line magic function `%!unzip` not found.\n"
     ]
    }
   ],
   "source": [
    "# !wget 'http://archive.ics.uci.edu/ml/machine-learning-databases/00406/Anuran Calls (MFCCs).zip'\n",
    "# !unzip Anuran*zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def noop(string):\n",
    "    return 0; # do nothing \n",
    "\n",
    "def convertSpecies(string, species):\n",
    "    \"\"\" check the passed in dictionary for the value and return it if it exists\n",
    "    otherwise, create a new entry \"\"\"\n",
    "    if string in species:\n",
    "        return species[string];\n",
    "    \n",
    "    if string not in species:\n",
    "        species[string] = len(species)\n",
    "        return len(species)\n",
    "\n",
    "def readAnuranFile(fileName):\n",
    "    species = {}\n",
    "    file = open(fileName,\"r\")\n",
    "    header = file.readline()\n",
    "    names = header.strip().split(',')\n",
    "\n",
    "    data = np.loadtxt(file ,delimiter=',', usecols=1+np.arange(25), converters={\n",
    "        24: lambda s: convertSpecies(s, species),\n",
    "        23: lambda s: noop(s),\n",
    "        22: lambda s: noop(s)\n",
    "    })\n",
    "    return (names, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MFCCs_ 1',\n",
       "  'MFCCs_ 2',\n",
       "  'MFCCs_ 3',\n",
       "  'MFCCs_ 4',\n",
       "  'MFCCs_ 5',\n",
       "  'MFCCs_ 6',\n",
       "  'MFCCs_ 7',\n",
       "  'MFCCs_ 8',\n",
       "  'MFCCs_ 9',\n",
       "  'MFCCs_10',\n",
       "  'MFCCs_11',\n",
       "  'MFCCs_12',\n",
       "  'MFCCs_13',\n",
       "  'MFCCs_14',\n",
       "  'MFCCs_15',\n",
       "  'MFCCs_16',\n",
       "  'MFCCs_17',\n",
       "  'MFCCs_18',\n",
       "  'MFCCs_19',\n",
       "  'MFCCs_20',\n",
       "  'MFCCs_21',\n",
       "  'MFCCs_22',\n",
       "  'Family',\n",
       "  'Genus',\n",
       "  'Species',\n",
       "  'RecordID'],\n",
       " array([[  1.52936298e-01,  -1.05585903e-01,   2.00721915e-01, ...,\n",
       "           0.00000000e+00,   1.00000000e+00,   1.00000000e+00],\n",
       "        [  1.71534257e-01,  -9.89747371e-02,   2.68425221e-01, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "        [  1.52317085e-01,  -8.29726739e-02,   2.87127957e-01, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "        ..., \n",
       "        [ -5.82556781e-01,  -3.43237394e-01,   2.94678468e-02, ...,\n",
       "           0.00000000e+00,   9.00000000e+00,   6.00000000e+01],\n",
       "        [ -5.19497158e-01,  -3.07553060e-01,  -4.92150002e-03, ...,\n",
       "           0.00000000e+00,   9.00000000e+00,   6.00000000e+01],\n",
       "        [ -5.08833178e-01,  -3.24105837e-01,   6.20676235e-02, ...,\n",
       "           0.00000000e+00,   9.00000000e+00,   6.00000000e+01]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Anames, Adata = readAnuranFile(\"Frogs_MFCCs.csv\")\n",
    "Anames, Adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "\n",
    "AXColumns = np.arange(26)\n",
    "\n",
    "AtargetColumn4 = Anames.index('RecordID')\n",
    "AXColumns = np.delete(AXColumns, AtargetColumn4)\n",
    "\n",
    "AtargetColumn = Anames.index(\"Species\")\n",
    "AXColumns = np.delete(AXColumns, AtargetColumn)\n",
    "\n",
    "AtargetColumn3 = Anames.index('Genus')\n",
    "AXColumns = np.delete(AXColumns, AtargetColumn3)\n",
    "\n",
    "AtargetColumn2 = Anames.index('Family')\n",
    "AXColumns = np.delete(AXColumns, AtargetColumn2)\n",
    "\n",
    "Tanuran = Adata[:, [AtargetColumn, AtargetColumn2, AtargetColumn3]].reshape((-1, 1)) # to keep 2-d matrix form\n",
    "Xanuran = Adata[:, AXColumns]\n",
    "\n",
    "AXnames = cp.deepcopy(Anames)\n",
    "AXnames.remove('Species')\n",
    "AXnames.remove('Family')\n",
    "AXnames.remove('Genus')\n",
    "AXnames.remove('RecordID')\n",
    "\n",
    "ATnames = ['Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7195, 22), (21585, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xanuran.shape, Tanuran.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1529363 , -0.1055859 ,  0.20072191,  0.31720106,  0.26076385,\n",
       "         0.10094464, -0.1500626 , -0.17112763,  0.12467644,  0.18865415,\n",
       "        -0.07562172, -0.15643593,  0.08224512,  0.13575204, -0.02401665,\n",
       "        -0.10835111, -0.07762252, -0.0095678 ,  0.05768398,  0.11868014,\n",
       "         0.01403845,  0.        ],\n",
       "       [ 0.17153426, -0.09897474,  0.26842522,  0.33867186,  0.2683531 ,\n",
       "         0.06083509, -0.22247464, -0.20769267,  0.17088287,  0.27095828,\n",
       "        -0.09500394, -0.25434147,  0.02278623,  0.1633201 ,  0.01202228,\n",
       "        -0.09097401, -0.05650952, -0.03530336,  0.02013996,  0.08226299,\n",
       "         0.02905574,  0.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xanuran[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tanuran[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7866 samples in class 0\n",
      "594 samples in class 1\n",
      "3532 samples in class 2\n",
      "332 samples in class 3\n",
      "560 samples in class 4\n",
      "1145 samples in class 5\n",
      "313 samples in class 6\n",
      "131 samples in class 7\n",
      "440 samples in class 8\n",
      "216 samples in class 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('{} samples in class {}'.format(np.sum(Tanuran==i), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results3 = trainNNs(Xanuran, Tanuran, 0.8, [0, 5, [5, 5]], 5, 100, classify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  [0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608],\n",
       "  [0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287],\n",
       "  0.2121257781982422],\n",
       " [5,\n",
       "  [0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608],\n",
       "  [0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287],\n",
       "  1.9051449298858643],\n",
       " [[5, 5],\n",
       "  [0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608,\n",
       "   0.42665971498088284,\n",
       "   0.08255126868265554,\n",
       "   0.17205422314911367,\n",
       "   0.0031282586027111575,\n",
       "   0.012165450121654502,\n",
       "   0.0033020507473062216,\n",
       "   0.005908932916232186,\n",
       "   0.0024330900243309003,\n",
       "   0.05179005908932916,\n",
       "   0.009384775808133473,\n",
       "   0.01338199513381995,\n",
       "   0.021897810218978103,\n",
       "   0.007994438651372959,\n",
       "   0.02259297879735836,\n",
       "   0.027632950990615225,\n",
       "   0.062391379909628085,\n",
       "   0.012165450121654502,\n",
       "   0.019117135905457074,\n",
       "   0.04344803614876608],\n",
       "  [0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287,\n",
       "   0.4260929909784872,\n",
       "   0.08258154059680778,\n",
       "   0.17210270645385148,\n",
       "   0.0034698126301179735,\n",
       "   0.012491325468424705,\n",
       "   0.0034698126301179735,\n",
       "   0.006245662734212352,\n",
       "   0.002081887578070784,\n",
       "   0.05135322692574601,\n",
       "   0.009715475364330326,\n",
       "   0.0131852879944483,\n",
       "   0.021512838306731435,\n",
       "   0.008327550312283136,\n",
       "   0.022900763358778626,\n",
       "   0.027758501040943788,\n",
       "   0.062456627342123525,\n",
       "   0.012491325468424705,\n",
       "   0.018736988202637056,\n",
       "   0.04302567661346287],\n",
       "  4.404841899871826]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.05263157894736842, 0.05263157894736842, 0.2121257781982422],\n",
       " [5, 0.05263157894736842, 0.05263157894736842, 1.9051449298858643],\n",
       " [[5, 5], 0.05263157894736842, 0.05263157894736842, 4.404841899871826]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0.05263157894736842, 0.05263157894736842, 0.2121257781982422]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestNetwork(summarize(results3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results4 = trainNNs(Xanuran, Tanuran, 0.8, [1, 5, [5, 5], 10, [3, 3, 3], [8, 8], 15, [2, 2, 2, 2], [10, 10], [5, 5, 5]], 10, 100, classify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0.05263157894736841, 0.052631578947368404, 1.6099069118499756]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestNetwork(summarize(results4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AXtrain,ATtrain,AXtest,ATtest = ml.partition(Xanuran, Tanuran, (0.8, 0.2), classification=True);\n",
    "AnBestnnet = nns.NeuralNetwork(AXtrain.shape[1], 1, ATtrain.shape[1]); # again using best network\n",
    "AnBestnnet.train(AXtrain, ATtrain, 100)\n",
    "\n",
    "APtrain,APrtrain = AnBestnnet.use(AXtrain,allOutputs=True)\n",
    "APtest,APrtest = AnBestnnet.use(AXtest,allOutputs=True)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.hstack((ATtrain,APtrain)), '.')\n",
    "plt.legend(('Actual','Predicted'))\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.hstack((ATtest,APtest)), '.')\n",
    "plt.legend(('Actual','Predicted'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph - I'm not really sure what is going on. it looks like after a certain amount of time the actual values go off the rails a bit. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
